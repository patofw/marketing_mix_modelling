{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHwiHTunM3IG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Marketing Mix Models \n",
        "\n",
        "## The Basics\n",
        "\n",
        "\n",
        "Marketing Mix Modeling (MMM or MMX) is a statistical analysis technique used to evaluate the impact of a company's marketing activities on sales (or any variable that wants to be measured). It relies in analysing historical data and typically fitting a model in which sales acts as a dependent variable.\n",
        "\n",
        "MMM helps businesses understand the effectiveness of different marketing channels (such as TV, online ads, promotions, and pricing) and how they interact with each other. This insight allows companies to allocate their marketing budgets more efficiently, optimize their marketing strategies. MMM is particularly useful because it quantifies the return on investment (ROI) of marketing expenditures, enabling data-driven decision-making.\n",
        "\n",
        "It's a highly demanded skill that can be applied in virtually, any industry. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective of this Notebook \n",
        "\n",
        "This notebook serves as a guide on how to build MM models in order to obtain contributions of the different channels (touchpoints) on product sales. This toy example includes relevant feature transformations as adstock (decay), seasonality, saturation, lags, etc. \n",
        "\n",
        "Likewise, it has been built with toy data which mimics real-life scenarios, but is not a full picture of the marketing spends a real product has during a year.\n",
        "\n",
        "The basic application of these type of models uses a traditional linear model, however, it does have a few limitations that will be explored across the Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hh_rnfspT4t"
      },
      "source": [
        "## 1. Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb5V0LWfWI_k"
      },
      "outputs": [],
      "source": [
        "# Imports.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "import optuna\n",
        "# from scipy.interpolate import make_interp_spline\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "# import dabl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up \n",
        "\n",
        "# Configuration for viz and verbosity.\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 8)\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Path were the input file is stored.\n",
        "data_path = \"../data/cough_and_cold_sales.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers\n",
        "# Data process .\n",
        "\n",
        "def get_median_filtered(signal: pd.Series, threshold: int = 3):\n",
        "    \n",
        "    \"\"\"\n",
        "    Función que nos permite sacar outliers utilizando el método de la mediana\n",
        "    filtrada. Es bastante robusto a ruido, por lo que es ideal para series \n",
        "    temporales. \n",
        "     \n",
        "    Directamente reemplaza outliers con la mediana!\n",
        "    Info sobre el método en \n",
        "    https://en.wikipedia.org/wiki/Median_filter \n",
        "    :param signal: una serie o señal\n",
        "    :param threshold: desviaciones respecto a la mediana    \n",
        "    :return:   array igual a serie con los outliers reemplazados. \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    outlier_check_signal = signal.copy()\n",
        "    difference = np.abs(outlier_check_signal - np.median(outlier_check_signal))\n",
        "    median_difference = np.median(difference)\n",
        "    if median_difference == 0:\n",
        "        s = 0\n",
        "    else:\n",
        "        s = difference / float(median_difference)\n",
        "    mask = s > threshold\n",
        "    outlier_check_signal[mask] = np.median(outlier_check_signal)\n",
        "    return outlier_check_signal\n",
        "\n",
        "def plot_outliers_signal(\n",
        "        signal: pd.Series, threshold: int = 2, return_mask: bool = True\n",
        ") -> None | np.ndarray:\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        signal (pd.Series): _description_\n",
        "        threshold (int, optional): _description_. Defaults to 3.\n",
        "        return_mask (bool, optional): _description_. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        None | np.ndarray: _description_\n",
        "    \"\"\"\n",
        "    kw = dict(marker='o', linestyle='none', color='r', alpha=0.35)\n",
        "\n",
        "    mds = get_median_filtered(signal, threshold=threshold)\n",
        "    outlier_idx = np.where(mds != signal)[0]\n",
        "    # make sure we use the series' index.\n",
        "    outlier_idx = signal.index[outlier_idx]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    plt.plot(signal, color = \"darkblue\")\n",
        "    plt.plot(\n",
        "        outlier_idx, signal[outlier_idx], **kw, \n",
        "        label = \"Outliers\"\n",
        "    )\n",
        "    plt.title(\"Outlier detection with cutoff {}\".format(threshold))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if return_mask: \n",
        "      return outlier_idx\n",
        "\n",
        "\n",
        "def _is_between(x, low, high) -> bool:\n",
        "    if x >= low and x <= high:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def add_flag(\n",
        "    df: pd.DataFrame,\n",
        "    low: int,\n",
        "    high: int,\n",
        "):\n",
        "    # add flag\n",
        "    flag = df.apply(\n",
        "        lambda row: \n",
        "            _is_between(\n",
        "                row.name, low, high  # usig the index of the df.\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    return flag\n",
        "\n",
        "\n",
        "# Carry over, Saturation helpers.\n",
        "\n",
        "def carry_over_effect(\n",
        "    input_variable: pd.Series, decay: float, lag: int\n",
        ") -> pd.Series:\n",
        "    \"Apply carry over transformation to a given pandas Series\"\n",
        "    output = []\n",
        "    for row in range(len(input_variable)):\n",
        "        # Take care of observations with less days than required for lags. \n",
        "        # e.g. For the first day, we have a problem if lag=5.\n",
        "        lag_aux = min(lag, row)\n",
        "        \n",
        "        # Apply decay effect to every row.\n",
        "        output.append(\n",
        "            sum(\n",
        "                [\n",
        "                    input_variable[row-i] * (decay ** i)  for i in range(lag_aux + 1)\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "    return pd.Series(output)\n",
        "\n",
        "def saturation_effect(input_variable: pd.Series, alpha: float\n",
        ") -> pd.Series:\n",
        "    \"Apply exponential transformation to a given pandas Series.\"\n",
        "    return (1 - np.exp(-alpha * input_variable))\n",
        "\n",
        "def adstock_tranformation(\n",
        "    input_variable: pd.Series, alpha: float, decay: float, lag: int\n",
        ") -> pd.Series:\n",
        "    \"Apply adstock transformation to a given pandas Series.\"\n",
        "    return saturation_effect(\n",
        "        carry_over_effect(\n",
        "            input_variable, decay, lag\n",
        "        ), alpha\n",
        "    )\n",
        "\n",
        "def add_adstock_to_pdf(\n",
        "    pdf_input: pd.DataFrame,\n",
        "    hyperparameters: dict,\n",
        "    channels: list,\n",
        ") -> pd.DataFrame:\n",
        "    \"Add adstock columns to a given dataset.\"\n",
        "    \n",
        "    pdf = pdf_input.copy()\n",
        "    pdf.reset_index(drop=True, inplace=True)\n",
        "    # Add a column for each channel with the name adstock_channel.\n",
        "    for c in channels:\n",
        "        pdf[f'adstock_{c}'] = adstock_tranformation(\n",
        "            pdf[c],\n",
        "            hyperparameters[f'alpha_{c}'],\n",
        "            hyperparameters[f'decay_{c}'],\n",
        "            hyperparameters[f'lag_{c}']\n",
        "        ).values\n",
        "    return pdf\n",
        "\n",
        "def sales_from(\n",
        "    channel: str,\n",
        "    features: list,\n",
        "    model: LinearRegression,\n",
        "    X: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"Compute sales attributed to a certain channel\"\n",
        "    coef = model.coef_[features.index(channel)]\n",
        "    obs = X.iloc[:, features.index(channel)]\n",
        "    return  coef * obs\n",
        "\n",
        "def create_stack_df(_df, features: list, lr: LinearRegression):\n",
        "    plot_df = pd.DataFrame()\n",
        "\n",
        "    for f in _df.columns:\n",
        "        if \"adstock\" in f:\n",
        "            _series = sales_from(f, features, lr, _df)\n",
        "        elif \"index\" in f:\n",
        "            _series = sales_from(f, features, lr, _df)\n",
        "\n",
        "        _series.name = f\n",
        "        plot_df = pd.concat([plot_df, _series], axis=1)\n",
        "    return plot_df\n",
        "\n",
        "\n",
        "\n",
        "def performance_metrics(true: np.ndarray, pred: np.ndarray) -> None:\n",
        "\n",
        "    r2 = r2_score(\n",
        "        true,\n",
        "        pred\n",
        "    )\n",
        "\n",
        "    mape = mean_squared_error(\n",
        "        true,\n",
        "        pred\n",
        "    )\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print()\n",
        "    print(f'MSE:{np.round(mape, 3)}')\n",
        "    print(f'R2: {np.round(r2, 3)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa_hhZj2GluZ"
      },
      "source": [
        "## 3. EDA and basic transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wShwfASJqTTe"
      },
      "source": [
        "Here we have synthetic weekly data of the sales of an off the counter **(OTC) cough and cold medicine** in a particular country. Along with the sales data, we have marketing spends for different channels that are typically relevant in the pharma industry. In this simple toy example we have a normalized monetary spend for tv, social media, medical crongresses, and trade (expenditure related to in-pharmacy display). We also have two external variables, the `stringency_index` which is related to the severness of COVID restrictions in this country, and the `flu_index` which is a variable monitored by the World Health Organization to track the intensity of the flu season in a particular region.  \n",
        "\n",
        "Our objective is to determine the ROI of each of our mtk channels, to enable data-drive decision making for allocating our future marketing budget. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define channels to use.\n",
        "CHANNELS: list[str] = ['tv', 'social_media', 'congress', 'trade']\n",
        "EXT_VARS: list[str] = ['flu_index', 'stringency_index'] \n",
        "# Define target variable.\n",
        "TARGET = 'sales'\n",
        "# Define weeks for testing period.\n",
        "TEST_SIZE = 8\n",
        "# Seed for reproducibility\n",
        "SEED = 123456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv(data_path, sep= \";\", parse_dates=[\"date\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set date as index as it will make plotting easier\n",
        "df.set_index(\"date\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the target var and its Outliers\n",
        "outlier_index = plot_outliers_signal(\n",
        "    df[TARGET], threshold=2\n",
        ")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have an evident abnormal event that happened in 2020 and 2021 that collapsed the sales. There's also a high spike in March 2020. This is of course, due to the pandemic, where there was a high \"panic buying\" spike, and later, a heavy drop in sales. \n",
        "\n",
        "What to do here?\n",
        "\n",
        "Many people decide to leave out the pandemic years. However, the effects of COVID could still be felt up until early 2022, which leaves us with very little data for fitting our model. What's best depends on the use case, however, for this example, let's add a flag that resembles the effects of the pandemic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding a Covid flag\n",
        "df['covid_flag'] = add_flag(\n",
        "    df,\n",
        "    outlier_index.min(),\n",
        "    outlier_index.max()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(2) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot it\n",
        "px.line(df.sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLot channel spend for month\n",
        "fig = px.bar(\n",
        "    df, \n",
        "    x=df.index, \n",
        "    y=CHANNELS, \n",
        "    title=\"Channel Spend per Date\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSo-JtkFp-Ue"
      },
      "source": [
        "## 4. Adstock modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GqTdpENqaXI"
      },
      "source": [
        "In this section we add (i) carry over and (ii) saturation effects.\n",
        "\n",
        "We want to model the relationship of the different investment channels using a linear regression, however we want to take into account the [decay effect](https://en.wikipedia.org/wiki/Advertising_adstock#Advertising_lag:_decay_effect) and the [law of diminishing returns](https://en.wikipedia.org/wiki/Advertising_adstock#Campaign_carry-over).\n",
        "\n",
        "---\n",
        "For this, instead of working directly with spend per channel, we apply different transformations to them\n",
        "\n",
        "> 1. For carry over $c_{t}$, we apply the following transformation.\n",
        "$$c_{t} = x_{t} + \\sum_{j=1}^{n} \\lambda^{t}  x_{t-j}$$\n",
        ">\n",
        "> Where $x_{t}$ is the investment in the channel for the $t$ period, $n$ represents the amount of periods to look back and $\\lambda$ represents the strength of the decay factor. This means that investments made in time $t$ will have still an impact of the following weeks, and this effect will decay over time. Both $n$ and $\\lambda$ will be parameters of the function.\n",
        "><br/>\n",
        "><br/>\n",
        ">2. For the saturation effect $s_{t}$, the following transformation is applied.\n",
        ">\n",
        ">$$s_{t} =1 -  e^{-\\alpha x_{t}} $$\n",
        ">\n",
        "> In this case, $\\alpha$ will be a parameter to input.\n",
        "\n",
        "<br/>\n",
        "\n",
        "In consequence, instead of modeling sales as a function of investment per channel, we fit a regression to the transformations described previously. The question now is which values should we use for **$n$, $\\lambda$ and $\\alpha$**? We can think of them as *hyperparameters of our model*. And then perform a *numerical optimization* to find values that minimize the mean squared error (or any metric of our interest) of the model.\n",
        "<br/>\n",
        "\n",
        "\n",
        "Lastly, data presents a heavy seasonality, one hot encoded variables were added for every month in the year.\n",
        "\n",
        "---\n",
        "\n",
        "For a complete explanation of the methods used in here, we refer the reader to page *154/510* of [Introduction to Algorithmic Marketing](https://algorithmicweb.files.wordpress.com/2018/07/algorithmic-marketing-ai-for-marketing-operations-r1-7g.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgp2qYycKSbL"
      },
      "source": [
        "## 5. Fit model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKjQa9oo9jqO"
      },
      "source": [
        "### Tune hyperparameters\n",
        "\n",
        "- We use optuna for hyperparameter tuning, but other libraries like `scipy.minimize` should work as well.\n",
        "- For every channel, we have three ($\\alpha$, $n$ and $\\lambda$) hyperparameters to tune.\n",
        "- We work with a traditional linear regression. -> prone to overfitting!!\n",
        "- We aim to minimize *MSE*.\n",
        "- We repeat 10000 trials for the optimization, caution! can take some time. Feel free to change `n_trials=100` to speed things up.\n",
        "- We split the dataset in train and test. Since we are working with time series we work with consecutives chunks of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for train test.\n",
        "df_train = df.iloc[: -TEST_SIZE, :]\n",
        "df_test = df.iloc[-TEST_SIZE:, :]\n",
        "\n",
        "# Define features and target variable.\n",
        "features = [\n",
        "    c for c in list(df.columns) if 'flag' in c or 'index' in c\n",
        "] + [f'adstock_{c}' for c in CHANNELS]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv9Ix4RDa_Bf"
      },
      "outputs": [],
      "source": [
        "# Usually helpers should be at the top of the notebook, but we put this one here\n",
        "# as it is a dictionary with multiple values to configure and play around with.\n",
        "\n",
        "def objective_tune_params(trial, _df: pd.DataFrame = df_train):\n",
        "    \n",
        "    # we need to optimize and learn alpha, decay and lag\n",
        "    # on the suggestions, if we have expert or prior knowledge we can inform this better\n",
        "    # however, for this exercise, we leave these defaults.\n",
        "    hyperparameters: dict = {\n",
        "        'alpha_tv': trial.suggest_float('alpha_tv', 0, 0.1),\n",
        "        'decay_tv': trial.suggest_float('decay_tv',  0, 0.5),\n",
        "        'lag_tv': trial.suggest_int('lag_tv', 0, 6),\n",
        "        'alpha_social_media': trial.suggest_float('alpha_social_media', 0, 0.1),\n",
        "        'decay_social_media': trial.suggest_float('decay_social_media',  0, 0.5),        \n",
        "        'lag_social_media': trial.suggest_int('lag_social_media', 0, 6),\n",
        "        'alpha_congress': trial.suggest_float('alpha_congress', 0, 0.1),\n",
        "        'decay_congress': trial.suggest_float('decay_congress',  0, 0.5),\n",
        "        'lag_congress': trial.suggest_int('lag_congress', 0, 6),\n",
        "        'alpha_trade': trial.suggest_float('alpha_trade', 0, 0.1),\n",
        "        'decay_trade': trial.suggest_float('decay_trade',  0, 0.5),\n",
        "        'lag_trade': trial.suggest_int('lag_trade', 0, 6)\n",
        "    }\n",
        "    \n",
        "    # Add adstock variables\n",
        "    df_adstock = add_adstock_to_pdf(_df, hyperparameters, CHANNELS)\n",
        "\n",
        "    # Define X and y.  \n",
        "    X = df_adstock[features]\n",
        "    y = df_adstock[TARGET]\n",
        "    \n",
        "    # Initialize regression class.\n",
        "    #lr = Ridge()\n",
        "    lr = LinearRegression()\n",
        "    \n",
        "    # Fit model and compute error (mean squared error).\n",
        "    lr.fit(X, y)\n",
        "    y_hat = lr.predict(X)\n",
        "    return np.mean((y - y_hat) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlQwdnqn5d-K"
      },
      "outputs": [],
      "source": [
        "# Perform the optimization.\n",
        "study_tune_params = optuna.create_study(\n",
        "    sampler=optuna.samplers.RandomSampler(seed=SEED)\n",
        ")\n",
        "study_tune_params.optimize(objective_tune_params, n_trials=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXZljqlCLnAO"
      },
      "source": [
        "### Run final model.\n",
        "With tuned hyperparameters, we train the final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check optimized params.\n",
        "study_tune_params.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0_J5daSq5BM",
        "outputId": "b9f7560c-e10d-41eb-f579-deb65efb9fa9"
      },
      "outputs": [],
      "source": [
        "# Best set of hyperparameters.\n",
        "hyperparameters = study_tune_params.best_params\n",
        "\n",
        "# We replicate adstock transformation, train test splitting and model fitting. \n",
        "# Add adstock columns. \n",
        "df_adstock_train  = add_adstock_to_pdf(df_train, hyperparameters, CHANNELS) \n",
        "df_adstock_test = add_adstock_to_pdf(df_test, hyperparameters, CHANNELS)\n",
        "\n",
        "# Define train and test data.\n",
        "X_train, y_train = df_adstock_train[features], df_adstock_train[TARGET]\n",
        "X_test, y_test = df_adstock_test[features], df_adstock_test[TARGET]\n",
        "\n",
        "# Initialize model.\n",
        "lr = LinearRegression()\n",
        "\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hSppHJmM62O"
      },
      "source": [
        "## 6. Model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "bKt9OphHrK19",
        "outputId": "e627137b-2dbc-4db5-a25e-4377f87a13f8"
      },
      "outputs": [],
      "source": [
        "# Line chart for performance.\n",
        "\n",
        "y_train.index = df_train.index  # adding index back for convenience\n",
        "\n",
        "# Plot it\n",
        "actual_vs_pred = pd.DataFrame(\n",
        "    {\n",
        "        \"actual\": y_train,\n",
        "        \"pred\": lr.predict(X_train),\n",
        "       \n",
        "    },\n",
        "    index=df_train.index\n",
        ")\n",
        "actual_vs_pred.plot(title=\"Actual vs prediction\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Training Performance\")\n",
        "performance_metrics(y_train, lr.predict(X_train))\n",
        "\n",
        "print(\"Test Performance\")\n",
        "performance_metrics(y_test, lr.predict(X_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WDHCLldNktY"
      },
      "source": [
        "## 7. Channel contribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btC6qdJdPeiw"
      },
      "source": [
        "We observe the marginal channel contribution and the return on investment per channel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFqLZGV1hmLe",
        "outputId": "3bb917ac-bdcb-4e34-f162-7dd7c3798127"
      },
      "outputs": [],
      "source": [
        "\n",
        "# base sales takes into account the intercept plus monthly effects.\n",
        "base = (\n",
        "    sum(\n",
        "        [\n",
        "            lr.coef_[\n",
        "                features.index(c)\n",
        "            ] * X_train.iloc[:, features.index(c)]\n",
        "            for c in features if 'adstock' not in c\n",
        "        ]\n",
        "    )\n",
        "      + [lr.intercept_] * len(df_train.index))\n",
        "\n",
        "# Computation of contribution S = B + b1*C1 + b2*C2 \n",
        "# Cnt_2 = (B+B1*C1+B2*C2) - (B+B1*C1) = B2*C2 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create contribution plot\n",
        "plot_df = create_stack_df(X_train, features, lr)\n",
        "# re adding base. \n",
        "base.name = \"base\"\n",
        "plot_df = pd.concat([base, plot_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "px.area(\n",
        "    plot_df,\n",
        "    title=\"Contribution Painting\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Compute ROI for every channel.\n",
        "for c in features:\n",
        "    if 'index' not in c and 'flag' not in c:\n",
        "        channel_share = np.round(sum(sales_from(c, features, lr, X_train)) / sum(lr.predict(X_train)), 2)\n",
        "        channel_roi = np.round(sum(sales_from(c, features, lr, X_train)) / sum(df_adstock_train[c.replace('adstock_', '')]), 2)\n",
        "        print(f'For {c}: share of total sales is {channel_share} and ROI is {channel_roi}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "px.bar(\n",
        "        y=lr.coef_,\n",
        "        x=X_train.columns,\n",
        "        title='Contribution bar plot',\n",
        "        labels={\n",
        "            \"x\": \"Feature\",\n",
        "            \"y\": \"Contribution\"\n",
        "        }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Discussion \n",
        "\n",
        "- Why are some channel contributions negative? Does this make sense from a business POV? \n",
        "- What about scaling the data?\n",
        "- What about seasonality?\n",
        "- What would happen if we try Log or Semi-log models? Would it be worth it?\n",
        "- Would it be worth it to use another type of model, for example tree-based algorithms?\n",
        "- What recommendations would you give to business with these results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iHwiHTunM3IG",
        "6Hh_rnfspT4t",
        "J_xqhVxApn4H",
        "Aa_hhZj2GluZ",
        "HSo-JtkFp-Ue",
        "MKjQa9oo9jqO",
        "iXZljqlCLnAO",
        "1hSppHJmM62O",
        "9WDHCLldNktY",
        "CJsR0BD2n5-D",
        "SqrY5dZSpJYP"
      ],
      "name": "Juan Prida: Marketing Mix Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
