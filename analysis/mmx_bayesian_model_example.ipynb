{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHwiHTunM3IG",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "# Bayesian Marketing Mix Models \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective of this Notebook \n",
        "\n",
        "This notebook serves as a guide on how to build MM models in order to obtain contributions of the different channels (touchpoints) on product sales. This toy example includes relevant feature transformations as adstock (decay), seasonality, saturation, lags, etc. \n",
        "\n",
        "Likewise, it has been built with toy data which mimics real-life scenarios, but is not a full picture of the marketing spends a real product has during a year.\n",
        "\n",
        "The basic application of these type of models uses a sophisticated Bayesian models using the famous [LightweightMMM](https://github.com/google/lightweight_mmm) library.\n",
        "\n",
        "## About Bayesian Models and Probabilistic Programming. \n",
        "\n",
        "This excellent [tutorial](https://juanitorduz.github.io/intro_pymc3/) made by the developers of [PYMC](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html) (one of the leading open-source Bayesian statistics libraries in Python), goes over the fundamentals of Bayesian Machine Learning and a few of its advantages over traditional (frequentist) approaches.\n",
        "\n",
        "\n",
        "## About the data. \n",
        "\n",
        "We are using the same dataset as in the [mmx_linear_model_example](mmx_linear_model_example.ipynb), find more details there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hh_rnfspT4t"
      },
      "source": [
        "## 1. Imports and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightWeight MMX\n",
        "\n",
        "Marketing Mix is much more advanced. It uses Bayesian models and it allows for seamless calculation of adstock, ROI and more. It also includes a powerful optimizer. It's a great starting point for advanced MMX models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the relevant modules of the lightweight_mmm library\n",
        "from lightweight_mmm import lightweight_mmm\n",
        "from lightweight_mmm import optimize_media\n",
        "from lightweight_mmm import plot\n",
        "from lightweight_mmm import preprocessing\n",
        "# Import jax.numpy and any other library we might need.\n",
        "import jax.numpy as jnp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = \"../data/cough_and_cold_sales.csv\"\n",
        "\n",
        "# Define channels to use.\n",
        "CHANNELS: list[str] = ['tv', 'social_media', 'congress', 'trade']\n",
        "EXT_VARS: list[str] = ['flu_index', 'stringency_index'] \n",
        "# Define target variable.\n",
        "TARGET = 'sales'\n",
        "# Define weeks for testing period.\n",
        "TEST_SIZE = 8\n",
        "# Seed for reproducibility\n",
        "SEED = 123456\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(data_path, sep= \";\", parse_dates=[\"date\"])\n",
        "df.set_index('date', inplace=True)\n",
        "# Lightweight wants the data in a different format\n",
        "\n",
        "media_data = df[CHANNELS].values\n",
        "extra_features = df[EXT_VARS].values\n",
        "target = df[TARGET].values.reshape(1, -1)[0, : , ]\n",
        "# We assign (arbitrarly in this example) a total cost per channel\n",
        "costs = jnp.array(\n",
        "    [\n",
        "        350,  # TV cost in hundreds of thousands\n",
        "        100,  # social media\n",
        "        200,  # congress\n",
        "        175,  # trade\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Split and scale data.\n",
        "split_point = len(df) - TEST_SIZE\n",
        "# Media data\n",
        "media_data_train = media_data[: split_point, ...]\n",
        "media_data_test = media_data[split_point: , ...]\n",
        "# Extra features\n",
        "extra_features_train = extra_features[: split_point , ...]\n",
        "extra_features_test = extra_features[split_point: , ...]\n",
        "# Target\n",
        "target_train = target[: split_point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling is essential for many modelling problems and this one is no exception.\n",
        "\n",
        "Lightweight MMM provides `CustomScaler` which behaves accordingly with sklearn scalers.\n",
        "\n",
        "In this case, for the `cough_and_cold_sales.csv` the data is ALREADY scaled, so no need to re-escale it, however, for demonstration, we are going to scale the cost data as well. \n",
        "\n",
        "**__NOTE__**\n",
        "\n",
        "---\n",
        "In most cases you will need 3 or 4 scalers. One scaler for the media data, one for the target and one for costs. Optionally if you are adding extra features those might need an extra scaler. It is very important that you save and \"carry with you\" those scalers throughout your MMM journey as LighweightMMM will allow you to re-insert these scalers at different points to ensure everything is always in the correct scale and results. If some results don't make sense, it might be a scaling problem.\n",
        "\n",
        "A few more details on CustomScaler usage:\n",
        "\n",
        "This scaler can be used in two fashions for both the multiplication and division operation.\n",
        "\n",
        "By specifying a value to use for the scaling operation.\n",
        "By specifying an operation used at column level to calculate the value for the\n",
        "actual scaling operation.\n",
        "\n",
        "Eg. if one wants to scale the dataset by multiply by 100 you can directly pass multiply_by=100. Value can also be an array of an appropriate shape by which to divide or multiply the data. But if you want to multiply by the mean value of each column, then you can pass multiply_operation=jnp.mean (or any other operation desired).\n",
        "\n",
        "Operation parameters have the upper hand in the cases where both values and operations are passed, values will be ignored in this case.\n",
        "\n",
        "Consult the full class documentation if you still need to know more.\n",
        "\n",
        "In this demo we divide the data on media, extra features and the target by their mean to ensure that the result has a mean of 1. This allows the model to be agnostic to the scale of the inputs (e.g. a user can use either the number of sales or the value of sales). The costs are not used in the model directly, they are only used to inform the prior distributions on the media variables (see the model documentation here). These costs have been scaled down by multiplying by 0.15 to reflect typical ranges in MMMs.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leaving commented examples of how the scalers for the other datasets will look. \n",
        "\n",
        "# media_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
        "# extra_features_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
        "# target_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean)\n",
        "cost_scaler = preprocessing.CustomScaler(divide_operation=jnp.mean, multiply_by=0.15)\n",
        "\n",
        "# media_data_train = media_scaler.fit_transform(media_data_train)\n",
        "# extra_features_train = extra_features_scaler.fit_transform(extra_features_train)\n",
        "# target_train = target_scaler.fit_transform(target_train)\n",
        "costs = cost_scaler.fit_transform(costs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "costs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Quality (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlations, variances, spend_fractions, variance_inflation_factors = preprocessing.check_data_quality(\n",
        "    media_data=media_data,\n",
        "    target_data=target,\n",
        "    cost_data=costs,\n",
        "    extra_features_data=extra_features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlations[0].round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We should always aim for values under 3 ideally, but surely under 5!\n",
        "variance_inflation_factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For any analysis that aims to analyse feature importance, it's primordial to check for correlation in the covariates (covariance) as it can obfuscate or obscure the true relationship between the independent variables and your target variable. Tests like VIF (Variance Inflation Factor) can help you spot covariance issues. In case you assess that the variables are heavily correlated, it's important to either merge them into one feature, or find a way to \"break\" the correlation and isolate their effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training the model\n",
        "\n",
        "The model executes multiple simulations using [Markov Chain Monte Carlo](https://num.pyro.ai/en/stable/mcmc.html) simulations, which require a \"warmup\" to facilitate convergence. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmm = lightweight_mmm.LightweightMMM(model_name=\"adstock\")\n",
        "\n",
        "number_warmup=1000\n",
        "number_samples=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# For replicability in terms of random number generation in sampling\n",
        "# reuse the same seed for different trainings.\n",
        "mmm.fit(\n",
        "    media=media_data_train,\n",
        "    media_prior=costs,\n",
        "    target=target_train,\n",
        "    extra_features=extra_features_train,\n",
        "    number_warmup=number_warmup,\n",
        "    number_samples=number_samples,\n",
        "    seed=SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mmm.print_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ideally we would never allow an RHat value of above 1.1, and we should aim for 0 divergences. If divergences are present, they invalidate our model!! We need to analyse what could cause them with some diagnonstics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot.plot_media_channel_posteriors(media_mix_model=mmm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Posteriors vs Priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot.plot_prior_and_posterior(media_mix_model=mmm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the organge line is close to the blue one, it means our \"priors\" are informative and well selected. If not, then we probably passed on uninformative or wrong priors. We can modify them and re-run the experiment if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# We fit the model and check its performance.\n",
        "plot.plot_model_fit(mmm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# We have to scale the test media data if we have not done so before.\n",
        "new_predictions = mmm.predict(\n",
        "    media=media_data_test,\n",
        "    extra_features=extra_features_test,\n",
        "    seed=SEED\n",
        ")\n",
        "new_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plot.plot_out_of_sample_model_fit(\n",
        "    out_of_sample_predictions=new_predictions,\n",
        "    out_of_sample_target=target[split_point:]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Media insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "media_contribution, roi_hat = mmm.get_posterior_metrics(cost_scaler=cost_scaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot.plot_media_baseline_contribution_area_plot(\n",
        "    media_mix_model=mmm,\n",
        "    fig_size=(30,10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot.plot_bars_media_metrics(\n",
        "    metric=media_contribution,\n",
        "    metric_name=\"Media Contribution Percentage\",\n",
        "    channel_names=CHANNELS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot.plot_bars_media_metrics(metric=roi_hat, metric_name=\"ROI hat\", channel_names=CHANNELS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KPI == incremental sales contribution\n",
        "plot.plot_response_curves(\n",
        "    media_mix_model=mmm, seed=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prices = jnp.ones(mmm.n_media_channels)\n",
        "n_time_periods = TEST_SIZE\n",
        "budget = jnp.sum(jnp.dot(prices, media_data.mean(axis=0))) * n_time_periods\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run optimization with the parameters of choice.\n",
        "solution, kpi_without_optim, previous_media_allocation = optimize_media.find_optimal_budgets(\n",
        "    n_time_periods=n_time_periods,\n",
        "    media_mix_model=mmm,\n",
        "    extra_features=extra_features_test[:n_time_periods],\n",
        "    budget=budget,\n",
        "    prices=prices,\n",
        "    seed=SEED\n",
        ")\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Obtain the optimal weekly allocation.\n",
        "optimal_buget_allocation = prices * solution.x\n",
        "optimal_buget_allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# similar renormalization to get previous budget allocation\n",
        "previous_budget_allocation = prices * previous_media_allocation\n",
        "previous_budget_allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Both numbers should be almost equal\n",
        "budget, jnp.sum(solution.x * prices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Plot out pre post optimization budget allocation and predicted target variable comparison.\n",
        "plot.plot_pre_post_budget_allocation_comparison(\n",
        "    media_mix_model=mmm, \n",
        "    kpi_with_optim=solution['fun'], \n",
        "    kpi_without_optim=kpi_without_optim,\n",
        "    optimal_buget_allocation=optimal_buget_allocation, \n",
        "    previous_budget_allocation=previous_budget_allocation, \n",
        "    figure_size=(10,10),\n",
        "    channel_names=CHANNELS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iHwiHTunM3IG",
        "6Hh_rnfspT4t",
        "J_xqhVxApn4H",
        "Aa_hhZj2GluZ",
        "HSo-JtkFp-Ue",
        "MKjQa9oo9jqO",
        "iXZljqlCLnAO",
        "1hSppHJmM62O",
        "9WDHCLldNktY",
        "CJsR0BD2n5-D",
        "SqrY5dZSpJYP"
      ],
      "name": "Juan Prida: Marketing Mix Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
